{
  "name": "spider-engine",
  "version": "0.1.3",
  "description": "Web crawling and scraping engine.",
  "main": "index.js",
  "scripts": {
    "test": "make test"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/d-oliveros/node-spider.git"
  },
  "keywords": [
    "crawler",
    "spider",
    "scraping",
    "scrape",
    "engine",
    "event-emitter"
  ],
  "author": {
    "name": "David Oliveros"
  },
  "license": "GPL",
  "bugs": {
    "url": "https://github.com/d-oliveros/node-spider/issues"
  },
  "homepage": "https://github.com/d-oliveros/node-spider",
  "dependencies": {
    "cheerio": "^0.17.0",
    "debug": "^1.0.4",
    "lodash": "^2.4.1",
    "q": "^1.0.1",
    "querystring": "^0.2.0",
    "request": "^2.40.0"
  },
  "devDependencies": {
    "chai": "^1.9.1",
    "mocha": "^1.21.4"
  },
  "readme": "# Spider\n\n[![NPM version](https://badge.fury.io/js/spider-engine.svg)](http://badge.fury.io/js/spider-engine)\n\nWeb crawling and scraping engine powered by NodeJS.\n\n## How to use\n\nTo create a new spider, you can do:\n\n```js\nvar Spider = require('spider-engine');\nvar spider = new Spider(`scraper` || `options`);\n\nspider.query(`queryParams`);\n```\n\nThe spider is an `EventEmitter`, so you can get the scraped results as they come in, by doing:\n\n```js\nspider.on('data', function (data) {\n\tresults = data.items;\n\t// ...do something with the results\n});\n\nspider.on('finish', function(data) {\n\tconsole.log('Spider finished with code '+data.code+'. ' + data.message);\n});\n```\n\n### API\n\n#### Spider(scraper:Function)\n\nCreates a new basic spider with the provided scraper.\n\n```js\n// example:\n\nvar Spider = require('spider-engine');\n\nvar spider = new Spider( function($) {\n\n\t// Get all the links in the page\n\tvar links = [];\n\t$('a').each( function(i, elem) {\n\t\tlinks.push( $(elem).attr('href') );\n\t});\n\n\treturn {\n\t\titems: links,\n\t};\n});\n\nspider.query('http://en.wikipedia.org/wiki/Web_scraping');\n\nspider.on('data', function(results) {\n\tconsole.log(results); // -> returned data from the scraper;\n});\n```\n\n#### Spider(options:Object)\n\nCreates a new spider engine with the provided parameters.\n\nThe following options are supported:\n\n- `urlTemplate ( Function(queryParams) )` The function used to build the query. If no function is provided, the query will be used as is, and the spider won't automatically jumpt to the next page. You can use underscore's _.template function to generate your query templates.\n- `scraper ( Function($) )` The function that will be used to process the response's HTML. This function must returns an object containing:\n  - `items (Array)` The items to be scraped. You can build your items freely, this is what the spider will emit when scraping the site.\n  - `more (Boolean)` If the `more` flag is set, the spider will request the next target. The next target is the same target with the `start` parameter increased by `windowSize`.\n- `proxy (String) (optional)` - The proxy address to use. If no proxy is provided, the local IP will be used instead.\n- `defaults (Object) (optional)` Default values when building the URL.\n- `headers (String) (optional)` The headers to be sent as part of the spider's requests.\n- `maxRetries (Number) (default: 100)` If our IP is blocked, re-try to scrape the results this amount of times.\n\n\n#### Spider::query(`queryParams`:`Object`)\n\n```js\nspider.query('Scraping in nodejs');\n```\n\nThe `queryParams` can be a string, or an object with the following properties:\n- `query (String)` The query string to use. If a urlTemplate is provided, this string will be available in the construction of the url, under the variable name `query`. If no urlTemplate is provided, this string will be used as is. In other words, if you do not provide a urlTemplate, make sure to put the whole URL here.\n- `windowSize (Number) (default: 100)` The window size to use.\n- `start (Number) (default: 0)` The starting value. If a urlTemplate is provided and the scraper function returns the \"more\" flag, this number will be increased by `windowSize`, and the spider will move to the next target (Which is a queryString built with the urlTemplate function provided)\n\n\n#### Spider::kill()\n\nStops the spider. This will also trigger the `finish` event.\n\n```js\nspider.kill();\n```\n\n### Events\n\nSpider inherits from `EventEmitter`, so the following events can be emitted from a spider:\n\n- `start` - The spider has started.\n- `move` - The spider started a HTTP request\n- `data` - The spider scraped and returned results\n- `ipBlocked` - Our IP gets rejected from the server (Useful for logging, or to handle IP changes. Just saying.)\n- `finish` - The spider has finished.\n\n\n## Tests\n\n`make test`\n\nCheers.\n",
  "readmeFilename": "README.md",
  "_id": "spider-engine@0.1.3",
  "dist": {
    "shasum": "66fb47e2e496e8b0d418998922b78ecfdcc92078"
  },
  "_from": "spider-engine@0.1.3",
  "_resolved": "https://registry.npmjs.org/spider-engine/-/spider-engine-0.1.3.tgz"
}
